{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzyClfgaJEbW"
      },
      "source": [
        "üöÄ **Join Our Speech Team at Sarvam AI!** üöÄ\n",
        "\n",
        "\n",
        "\n",
        "We at [Sarvam AI](https://www.sarvam.ai/) are on a mission to revolutionize the GenAI landscape in India, and we're looking for talented **Data Engineers** and **ML Engineers** to help us achieve this. Become part of a team dedicated to building state-of-the-art ML systems for Speech Recognition and Text-to-Speech applications in Indian languages.\n",
        "\n",
        "\n",
        "\n",
        "**Why Join Our Speech Team?**\n",
        "\n",
        "- üß† **Develop Cutting-Edge ML Models:** Work on the forefront of machine learning to develop systems that understand and speak multiple Indian languages.\n",
        "\n",
        "- üìä **Handle Large-Scale Data Sets:** Dive into dataset curation, managing and processing over 1M hours of audio data to train our models.\n",
        "\n",
        "- üíª **Advanced GPU Infrastructure:** Get hands-on experience with our massive A100 and H100 GPU cluster, pushing the boundaries of what AI can achieve.\n",
        "\n",
        "- üõ†Ô∏è **Solve Real-World Problems:** Tackle hard applied research and engineering challenges with a direct impact on our products.\n",
        "\n",
        "- üåê **Work with Top Talent:** Collaborate with some of India's best ML Engineers and Product Developers in an environment that values innovation and creativity.\n",
        "\n",
        "\n",
        "\n",
        "**We're Excited to Offer the Following Opportunities (Full Time only):**\n",
        "\n",
        "- üå± **Summer Internship (2 months on-site/remote, earning up to 50k per month):** Ideal for freshers or students with a foundational grasp of ML and programming.\n",
        "\n",
        "  - **Data Engineer:** Engage in web scraping, manage distributed data processing, and develop robust data pipelines.\n",
        "\n",
        "  - **ML Engineer:** Train, monitor and eval state-of-the-art speech models.\n",
        "\n",
        "  \n",
        "\n",
        "- üî• **AI Residency (6 months on-site, earning up to 1L per month):** Perfect for those with professional experience or significant expertise. Contribute to our groundbreaking projects and refine your skills.\n",
        "\n",
        "  - **Data Engineer:** Oversee large-scale data mining and sophisticated engineering tasks for massive data flows.\n",
        "\n",
        "  - **ML Engineer:** Train advanced speech models on powerful GPU clusters using frameworks like PyTorch and JAX, and tools like NeMo and HuggingFace Transformers.\n",
        "\n",
        "\n",
        "\n",
        "**About Sarvam AI:**\n",
        "\n",
        "Sarvam AI is a well-funded GenAI startup focused on creating full-stack GenAI systems and applications tailored for India. Based out of Bangalore and Chennai, Sarvam AI is a place for those who are passionate about driving significant advancements through Generative AI, have a love for Indian languages, and are eager to make a substantial impact.\n",
        "\n",
        "\n",
        "\n",
        "**Join Us!**\n",
        "\n",
        "Complete our [Hiring Challenge in the Colab Notebook](https://colab.research.google.com/drive/1EiiLTf5zB8Jm2PxdU3H20rWUr40FrsGM?usp=sharing) to showcase your skills! After completing the challenge, please apply through this [Google Form](https://forms.gle/yP4Vd9QhiETTqEtW8). We‚Äôre eager to see your innovative solutions and potentially welcome you aboard to help shape the future of AI in India.\n",
        "\n",
        "\n",
        "\n",
        "*Note: Admissions are on a rolling basis until all positions are filled. Apply early to secure your spot in our team!*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97jDW6dhYI0_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upoYo6G_-5kf"
      },
      "source": [
        "# üåü Welcome to the Speech Team Hiring Challenge! üöÄ\n",
        "\n",
        "Hey there! We're thrilled to kick off this exciting challenge with two awesome tasks tailored to test your prowess in speech and text data analysis. These tasks are crucial for our hiring process and mirror the real-world scenarios our team loves to tackle. üéØ\n",
        "\n",
        "**Task 1: Semantic Chunking of a YouTube Video** üìπ\n",
        "- Dive into extracting meaningful audio-text pairs from a specific video. Show us your skill in achieving precise segmentation and alignment!\n",
        "\n",
        "**Task 2: Exploratory Data Analysis of New Testament Audio and Text** üìñ\n",
        "- Get your hands dirty with a deep dive into the audio and text from the New Testament in your mother tongue. We're looking for sharp insights that could revolutionize text-to-speech and speech-to-text technologies.\n",
        "\n",
        "Please tackle both tasks with your full creativity and analytical skills as they are equally important in our evaluation. ü§ì üèãÔ∏è‚Äç‚ôÇÔ∏è Your innovative approaches, depth of analysis, and tech-savviness will be key to understanding how well you fit into our dynamic team.\n",
        "\n",
        "**Submission Instructions:**\n",
        "- Make sure to create a copy of your Google Colab notebook for each task.\n",
        "- Set the notebook to **shareable** and grant viewing access to **abhigyan@sarvam.ai**.\n",
        "- Once you've perfected your work, please paste the link to your notebook in the [Google Form provided](https://forms.gle/qxy2LF4Jtph7xhYHA). This step is crucial for us to review your submissions properly.\n",
        "\n",
        "Good luck, and let's see what amazing things you can uncover! üåàüëÄ\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzqLZrA2UEmL"
      },
      "source": [
        "## Task 1: Semantic Chunking of a Youtube Video\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "The objective is to extract high-quality, meaningful (semantic) segments from the specified YouTube video: [Watch Video](https://www.youtube.com/watch?v=Sby1uJ_NFIY).\n",
        "\n",
        "Suggested workflow:\n",
        "1. **Download Video and Extract Audio:** Download the video and separate the audio component.\n",
        "2. **Transcription of Audio:** Utilize an open-source Speech-to-Text model to transcribe the audio. *Provide an explanation of the chosen model and any techniques used to enhance the quality of the transcription.*\n",
        "3. **Time-Align Transcript with Audio:** *Describe the methodology and steps for aligning the transcript with the audio.*\n",
        "4. **Semantic Chunking of Data:** Slice the data into audio-text pairs, using both semantic information from the text and voice activity information from the audio, with each audio-chunk being less than 15s in length. *Explain the logic used for semantic chunking and discuss the strengths and weaknesses of your approach.*\n",
        "\n",
        "**Judgement Criteria:**\n",
        "\n",
        "1. **Precision-Oriented Evaluation:** The evaluation focuses on precision rather than recall. Higher scores are achieved by reporting fewer but more accurate segments rather than a larger number of segments with inaccuracies. Segment accuracy is determined by:\n",
        "   - **Transcription Quality:** Accuracy of the text transcription for each audio chunk.\n",
        "   - **Segment Quality:** Semantic richness of the text segments.\n",
        "   - **Timestamp Accuracy:** Precision of the start and end times for each segment. Avoid audio cuts at the start or end of a segment.\n",
        "   \n",
        "2. **Detailed Explanations:** Provide reasoning behind each step in the process.\n",
        "3. **Generalization:** Discuss the general applicability of your approach, potential failure modes on different types of videos, and adaptation strategies for other languages.\n",
        "4. **[Bonus-1]** **Gradio-app Interface:** Wrap your code in a gradio-app which takes in youtube link as input and displays the output in a text-box.\n",
        "5. **[Bonus-2]** **Utilizing Ground-Truth Transcripts:** Propose a method to improve the quality of your transcript using a ground-truth transcript provided as a single text string. Explain your hypothesis for this approach. *Note that code-snippet isn't required for this question.*\n",
        "\n",
        "  As an example - for the audio extracted from [yt-link](https://www.youtube.com/watch?v=ysLiABvVos8), how can we leverage transcript scraped from [here](https://www.newsonair.gov.in/bulletins-detail/english-morning-news-7/), to improve the overall transcription quality of segments?\n",
        "\n",
        "**Submission Format:**\n",
        "\n",
        "Your submission should be a well-documented Jupyter notebook capable of reproducing your results. The notebook should automatically install all required dependencies and output the results in the specified format.\n",
        "\n",
        "- **Output Format:** Provide the results as a list of dictionaries, each representing a semantic chunk. Each dictionary should include:\n",
        "  - `chunk_id`: A unique identifier for the chunk (integer).\n",
        "  - `chunk_length`: The duration of the chunk in seconds (float).\n",
        "  - `text`: The transcribed text of the chunk (string).\n",
        "  - `start_time`: The start time of the chunk within the video (float).\n",
        "  - `end_time`: The end time of the chunk within the video (float).\n",
        "\n",
        "```python\n",
        "sample_output_list = [\n",
        "    {\n",
        "        \"chunk_id\": 1,\n",
        "        \"chunk_length\": 14.5,\n",
        "        \"text\": \"Here is an example of a semantic chunk from the video.\",\n",
        "        \"start_time\": 0.0,\n",
        "        \"end_time\": 14.5,\n",
        "    },\n",
        "    # Additional chunks follow...\n",
        "]\n",
        "```\n",
        "\n",
        "Ensure that your code is clear, well-commented, and easy to follow, with explanations for each major step and decision in the process. The notebook should be able to install all the dependencies automatically and generate the reported output when run.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gl-lp90ilqZ"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!curl https://raw.githubusercontent.com/readbeyond/aeneas/master/install_dependencies.sh -o install_dependencies.sh\n",
        "!bash install_dependencies.sh\n",
        "%pip install yt-dlp moviepy SpeechRecognition SpeechRecognition[whisper-local] tiktoken semchunk aeneas gradio\n",
        "\n",
        "# imports\n",
        "import yt_dlp as youtube_dl\n",
        "import pathlib\n",
        "import gradio as gr\n",
        "import speech_recognition as sr\n",
        "import tiktoken\n",
        "import semchunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwHhtf8_i7nC"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "transcript_path = \"transcript.txt\"\n",
        "audio_path = \"audio.wav\"\n",
        "video_path = \"video.mp4\"\n",
        "video_url = \"\"\n",
        "syncmap_path = \"syncmap.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W00nRD-WTTXR"
      },
      "outputs": [],
      "source": [
        "# Your code for Task 1 goes here!\n",
        "\n",
        "# Given url\n",
        "vid_url = \"https://www.youtube.com/watch?v=Sby1uJ_NFIY\"\n",
        "\n",
        "def download_save_video(url, save_path):\n",
        "    ydl_opts = {\n",
        "        'format': 'best',\n",
        "        'outtmpl': save_path\n",
        "    }\n",
        "    # only download if the file does not exist\n",
        "    if pathlib.Path(save_path).exists() is False:\n",
        "        print(f\"Downloading video from url:{url} at {save_path}\")\n",
        "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "        print(\"Successfully downloaded the video!\")\n",
        "    else:\n",
        "      print(\"Video already exists... Skipping.\")\n",
        "\n",
        "# using moviepy to extract audio from video\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def extract_and_write_audio(video_path, audio_path):\n",
        "    if pathlib.Path(video_path) is False:\n",
        "        print(\"Video file does not exist! Exiting...\")\n",
        "        return\n",
        "    if pathlib.Path(\"audio.mp3\").exists():\n",
        "        print(\"Audio file already exists! Exiting...\")\n",
        "        return\n",
        "    video = VideoFileClip(video_path)\n",
        "    audio = video.audio\n",
        "    print(\"Extracting audio from the video and writing it to the disk...\")\n",
        "    audio.write_audiofile(audio_path)\n",
        "    print(\"Successfully written\")\n",
        "\n",
        "\n",
        "def download_and_extract_audio(url):\n",
        "  download_save_video(url,video_path)\n",
        "  extract_and_write_audio(video_path,audio_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLDNgqi9331w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def use_whisper_openai(audio_path):\n",
        "    print(\"Initializing Whisper...\")\n",
        "    r = sr.Recognizer()\n",
        "\n",
        "    with sr.AudioFile(audio_path) as source:\n",
        "        print(\"Listening to audio...\")\n",
        "        audio = r.record(source)\n",
        "    return r.recognize_whisper(audio)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def chunk_text(text):\n",
        "    chunk_size = 15\n",
        "    encoder = tiktoken.encoding_for_model('gpt-4')\n",
        "    token_counter = lambda text: len(encoder.encode(text))\n",
        "    chunked = semchunk.chunk(text,chunk_size,token_counter)\n",
        "\n",
        "    end_string = \"\"\n",
        "\n",
        "    for line in chunked:\n",
        "        end_string += line + \"\\n\"\n",
        "\n",
        "    return end_string\n",
        "\n",
        "def return_semantic_rich_script(audio_path):\n",
        "    return chunk_text(use_whisper_openai(audio_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Nz7LKiFxeQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R42jdLnh9VzV"
      },
      "outputs": [],
      "source": [
        "import aeneas\n",
        "from aeneas.executetask import ExecuteTask\n",
        "from aeneas.task import Task\n",
        "\n",
        "def execute_aeneas(transcript_path, audio_path, syncmap_path,text_openai):\n",
        "\n",
        "  with open(transcript_path,'w+') as file:\n",
        "    file.write(text_openai)\n",
        "  print(\"Starting aeneas task...\")\n",
        "  config_string = u\"task_language=eng|is_text_type=plain|os_task_file_format=json\"\n",
        "  task = Task(config_string=config_string)\n",
        "\n",
        "  task.audio_file_path_absolute = audio_path\n",
        "  task.text_file_path_absolute = transcript_path\n",
        "  task.sync_map_file_path_absolute = syncmap_path\n",
        "\n",
        "  # Process the Task\n",
        "  ExecuteTask(task).execute()\n",
        "\n",
        "  # Output the sync map to a file\n",
        "  task.output_sync_map_file()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a1_g_HRQ5sb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rewrite_new_syncmap(syncmap_path):\n",
        "  import json\n",
        "\n",
        "  # Load the original syncmap\n",
        "  with open(syncmap_path, 'r') as f:\n",
        "      original_syncmap = json.load(f)\n",
        "\n",
        "  # If original_syncmap is a dictionary, get the list from it\n",
        "  if isinstance(original_syncmap, dict):\n",
        "      # Assuming the list is under the key 'fragments'\n",
        "      original_syncmap = original_syncmap.get('fragments', [])\n",
        "\n",
        "  # If original_syncmap is a string, parse it as JSON\n",
        "  elif isinstance(original_syncmap, str):\n",
        "      original_syncmap = json.loads(original_syncmap)\n",
        "\n",
        "  # Convert to the desired format\n",
        "  converted_syncmap = []\n",
        "  chunk_id = 1\n",
        "  for item in original_syncmap:\n",
        "      start_time = float(item['begin'])\n",
        "      end_time = float(item['end'])\n",
        "      text = ' '.join(item['lines'])\n",
        "\n",
        "      while end_time - start_time > 15:\n",
        "          # If the chunk is too long, split it\n",
        "          converted_item = {\n",
        "              \"chunk_id\": chunk_id,\n",
        "              \"chunk_length\": 15,\n",
        "              \"text\": text[:100],  # Assuming an average speaking rate of 150 wpm, 100 characters should be less than 15 seconds\n",
        "              \"start_time\": start_time,\n",
        "              \"end_time\": start_time + 15,\n",
        "          }\n",
        "          converted_syncmap.append(converted_item)\n",
        "          start_time += 15\n",
        "          text = text[100:]\n",
        "          chunk_id += 1\n",
        "\n",
        "      # Add the remaining part of the chunk\n",
        "      converted_item = {\n",
        "          \"chunk_id\": chunk_id,\n",
        "          \"chunk_length\": round(end_time - start_time, 2),\n",
        "          \"text\": text,\n",
        "          \"start_time\": start_time,\n",
        "          \"end_time\": end_time,\n",
        "      }\n",
        "      converted_syncmap.append(converted_item)\n",
        "      chunk_id += 1\n",
        "\n",
        "  # Write the converted syncmap to a new file\n",
        "  with open(syncmap_path, 'w') as f:\n",
        "      json.dump(converted_syncmap, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXW7heRISacJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "m07mgWf2Nb1N",
        "outputId": "d36f7fe4-1675-4d7f-ffb1-b5a37c3bb246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMPORTANT: You are using gradio version 3.36.1, however version 4.29.0 is available, please upgrade.\n",
            "--------\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://e32cfc8e11f34dc3eb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e32cfc8e11f34dc3eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def start(url):\n",
        "  download_and_extract_audio(url)\n",
        "  transcript = return_semantic_rich_script(audio_path)\n",
        "  print(transcript)\n",
        "  execute_aeneas(transcript_path,audio_path,syncmap_path,transcript)\n",
        "  rewrite_new_syncmap(syncmap_path)\n",
        "  syncmap_text = \"\"\n",
        "  with open(syncmap_path,'r') as file:\n",
        "    syncmap_text = file.read()\n",
        "  print(syncmap_text)\n",
        "  return syncmap_text\n",
        "\n",
        "iface = gr.Interface(fn=start,inputs=\"text\",outputs=\"text\")\n",
        "iface.launch(share=True,debug = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}